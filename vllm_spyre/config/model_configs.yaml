# Model Configuration Schema
# This file defines all supported models and their configurations
#
# Each model entry contains:
# - architecture: Pattern for matching HuggingFace model configs
# - supported_configs: List of runtime configurations
#   - Static batching configs: Must have 'warmup_shapes'
#   - Continuous batching configs: Must have 'max_model_len' and 'max_num_seqs'
# - device_configs: Device-specific settings keyed by tensor parallel size (only for CB)

models:
  # Granite 3.3 8B Instruct
  ibm-granite/granite-3.3-8b-instruct:
    architecture:
      model_type: granite
      num_hidden_layers: 40
      max_position_embeddings: 131072
      hidden_size: 4096
      vocab_size: 49159
      num_key_value_heads: 8
      num_attention_heads: 32

    supported_configs:
      # Static batching configurations
      - tp_size: 1
        warmup_shapes: [[2048, 1024, 16]]
      - tp_size: 4
        warmup_shapes: [[6144, 2048, 1]]
      - tp_size: 4
        warmup_shapes: [[7168, 1024, 4]]
      # Continuous batching configurations
      - tp_size: 1
        max_model_len: 3072
        max_num_seqs: 16
      - tp_size: 1
        max_model_len: 8192
        max_num_seqs: 4
      - tp_size: 2
        max_model_len: 8192
        max_num_seqs: 4
      - tp_size: 4
        max_model_len: 32768
        max_num_seqs: 32

    device_configs:
      4:  # TP size 4
        env_vars:
          VLLM_DT_MAX_BATCH_TKV_LIMIT: 131072  # 128k
          FLEX_HDMA_P2PSIZE: 268435456  # 256MB
          FLEX_HDMA_COLLSIZE: 33554432  # 32MB
        num_gpu_blocks_override:
          torch_sendnn_lt_1_0_3: 2080
          default: 8192
        chunked_prefill_config:
          max_num_batched_tokens: 1024

  # Granite 3.3 8B Instruct FP8
  ibm-granite/granite-3.3-8b-instruct-FP8:
    architecture:
      model_type: granite
      vocab_size: 49159
      quantization_config:
        format: float-quantized

    supported_configs:
      # Continuous batching configurations only
      - tp_size: 1
        max_model_len: 3072
        max_num_seqs: 16
      - tp_size: 4
        max_model_len: 16384
        max_num_seqs: 4
      - tp_size: 4
        max_model_len: 32768
        max_num_seqs: 32

    device_configs:
      4:
        env_vars:
          VLLM_DT_MAX_BATCH_TKV_LIMIT: 131072
          FLEX_HDMA_P2PSIZE: 268435456
          FLEX_HDMA_COLLSIZE: 33554432  # 32MB
        num_gpu_blocks_override:
          torch_sendnn_lt_1_0_3: 2080
          default: 8192
        chunked_prefill_config:
          max_num_batched_tokens: 1024

  # Granite 4 8B Dense
  ibm-granite/granite-4-8b-dense:
    architecture:
      model_type: granitemoehybrid
      num_hidden_layers: 40
      num_experts_per_tok: 0  # dense model
      max_position_embeddings: 131072
      hidden_size: 4096
      vocab_size: 100352
      num_key_value_heads: 8
      num_attention_heads: 32

    supported_configs:
      # Static batching configurations
      - tp_size: 1
        warmup_shapes: [[2048, 1024, 16]]
      - tp_size: 4
        warmup_shapes: [[6144, 2048, 1]]
      - tp_size: 4
        warmup_shapes: [[7168, 1024, 4]]
      # Continuous batching configurations
      - tp_size: 1
        max_model_len: 3072
        max_num_seqs: 16
      - tp_size: 1
        max_model_len: 8192
        max_num_seqs: 4
      - tp_size: 2
        max_model_len: 8192
        max_num_seqs: 4
      - tp_size: 4
        max_model_len: 32768
        max_num_seqs: 32

    device_configs:
      4:
        env_vars:
          VLLM_DT_MAX_BATCH_TKV_LIMIT: 131072
          FLEX_HDMA_P2PSIZE: 268435456
          FLEX_HDMA_COLLSIZE: 33554432  # 32MB
        num_gpu_blocks_override:
          torch_sendnn_lt_1_0_3: 2080
          default: 8192
        chunked_prefill_config:
          max_num_batched_tokens: 1024

  # Embedding models (static batching only, no device configs needed)
  ibm-granite/granite-embedding-125m-english:
    architecture:
      model_type: roberta
      num_hidden_layers: 12
      vocab_size: 50265

    supported_configs:
      - tp_size: 1
        warmup_shapes: [[512, 0, 64]]

  ibm-granite/granite-embedding-278m-multilingual:
    architecture:
      model_type: xlm-roberta
      num_hidden_layers: 12
      vocab_size: 250002

    supported_configs:
      - tp_size: 1
        warmup_shapes: [[512, 0, 64]]

  # Other supported models (static batching only)
  intfloat/multilingual-e5-large:
    architecture:
      model_type: xlm-roberta
      num_hidden_layers: 24
      vocab_size: 250002

    supported_configs:
      - tp_size: 1
        warmup_shapes: [[512, 0, 64]]

  BAAI/bge-reranker-v2-m3:
    architecture:
      model_type: xlm-roberta
      max_position_embeddings: 8194
      num_hidden_layers: 24
      vocab_size: 250002

    supported_configs:
      - tp_size: 1
        warmup_shapes: [[8192, 0, 1]]

  BAAI/bge-reranker-large:
    architecture:
      model_type: xlm-roberta
      max_position_embeddings: 514
      num_hidden_layers: 24
      vocab_size: 250002

    supported_configs:
      - tp_size: 1
        warmup_shapes: [[512, 0, 64]]

  sentence-transformers/all-roberta-large-v1:
    architecture:
      model_type: roberta
      num_hidden_layers: 24
      vocab_size: 50265

    supported_configs:
      - tp_size: 1
        warmup_shapes: [[128, 0, 8]]

# Made with Bob
