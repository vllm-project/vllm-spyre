name: Test
description: "Runs vLLM Spyre Unit Tests"

inputs:
  # environment
  python_version:
    description: "Version of python to install"
    type: string
    required: false
    default: "3.12"

  # vLLM version setup
  vllm_version_name:
    description: "Descriptive name of the vllm version to use for this run"
    type: string
    required: false
  vllm_version_repo:
    description: "The 'git+https' version identifier for vllm, to be used with 'uv add' to install a specific vllm version"
    type: string
    required: false
  
  # Pytest arguments
  markers:
    description: "Pytest markers to run"
    type: string
    required: true
  flags:
    description: "Extra pytest flags to apply"
    type: string
    required: false
  
  # Models under test
  hf_model:
    description: "HF format model name for the model to load from cache for testing"
    type: string
    required: false
    default: "ibm-ai-platform/micro-g3.3-8b-instruct-1b"
  hf_model_rev: 
    description: "The HF revision of the model to load from cache for testing"
    type: string
    required: false
    default: "6e9c6465a9d7e5e9fa35004a29f0c90befa7d23f"
  hf_model_2:
    description: "An optional second model if the suite needs to test two models"
    type: string
    required: false
  hf_model_2_rev: 
    description: "Revision for the optional second model"
    type: string
    required: false

  
runs:
  using: "composite"
  steps:

    - name: "Get changed source files"
      id: changed-src-files
      uses: tj-actions/changed-files@v46
      with: # Avoid using single or double quotes for multiline patterns
        files: |
          .github/workflows/test.yml
          pyproject.toml
          uv.lock
          tests/**/*.py
          vllm_spyre/**/*.py
          vllm_spyre/config/known_model_configs.json
          vllm_spyre/config/supported_configs.yaml

    - name: "Install PyTorch 2.7.1"
      if: steps.changed-src-files.outputs.any_changed == 'true'
      shell: bash
      run: |
        pip install torch=="2.7.1+cpu" --index-url https://download.pytorch.org/whl/cpu

    - name: "Install uv"
      if: steps.changed-src-files.outputs.any_changed == 'true'
      uses: astral-sh/setup-uv@v5
      with:
        version: "latest"
        python-version: ${{ inputs.python_version }}
        enable-cache: true
        ignore-nothing-to-cache: true
        cache-dependency-glob: |
          pyproject.toml

    - name: "Set vLLM version"
      if: ( steps.changed-src-files.outputs.any_changed == 'true' && inputs.vllm_version.repo )
      shell: bash
      run: |
        uv add ${{ inputs.vllm_version_repo }}
        echo "TEST_VLLM_VERSION=${{ inputs.vllm_version_name }}" >> "$GITHUB_ENV"

    - name: "Install vLLM with Spyre plugin"
      if: steps.changed-src-files.outputs.any_changed == 'true'
      shell: bash
      run: |
        uv venv .venv --system-site-packages
        source .venv/bin/activate

        # Syncs both the runtime and dev deps, based on the lockfile contents
        uv sync --frozen
        # Builds and installs the vllm_spyre wheel into .venv
        # This needs to be done after `uv sync`, or the wheel install will be
        # overwritten.
        uv pip install -v .

    - name: "Standardize HF model names for caching"
      id: standardize-names
      if: steps.changed-src-files.outputs.any_changed == 'true'
      shell: bash
      run: |
        # replace '/' characters in HF_MODEL with '--' for GHA cache keys and
        # in model file names in local HF hub cache
        
        model="${{ inputs.hf_model }}"
        revision="${{ inputs.hf_model_rev }}"

        safe_name="${model//\//--}"
        echo "model_key=${safe_name}_${revision}"              >> "$GITHUB_ENV"
        echo "model_path=${HF_HUB_CACHE}/models--${safe_name}" >> "$GITHUB_ENV"
        
        if [[ -n "${{ inputs.hf_model_2 }}" ]]; then
          model_2="${{ inputs.hf_model_2 }}"
          revision_2="${{ inputs.hf_model_2_rev}}"
          safe_name_2="${model_2//\//--}"
          echo "model_2_key=${safe_name_2}_${revision_2}"            >> "$GITHUB_ENV"
          echo "model_2_path=${HF_HUB_CACHE}/models--${safe_name_2}" >> "$GITHUB_ENV"
        fi

    - name: "Restore HF models cache"
      id: cache_restore
      if: steps.changed-src-files.outputs.any_changed == 'true'
      uses: actions/cache/restore@v4
      with:
        path: ${{ env.model_path }}
        key: ${{ runner.os }}-hf-model-${{ env.model_key }}

    - name: "Restore HF models cache for additional model"
      id: cache_restore_2
      if: ( steps.changed-src-files.outputs.any_changed == 'true' && inputs.hf_model_2 )
      uses: actions/cache/restore@v4
      with:
        path: ${{ env.model_2_path }}
        key: ${{ runner.os }}-hf-model-${{ env.model_2_key }}

    - name: "Download HF models"
      if: ( steps.changed-src-files.outputs.any_changed == 'true' && (steps.cache_restore.outputs.cache-hit != 'true' || steps.cache_restore_2.outputs.cache-hit != 'true'))
      shell: bash
      run: |
        # We are caching HF models (HF_HUB_CACHE) for reliability rather than
        # speed, since HF downloads are flaky for concurrent jobs.
        # Be careful when adding models to the cache here, as the GHA cache is
        # limited to 10 GB.
        # If a new model is added here, a new hash key is generated. The
        # previous cache blob can then be removed by an admin or can be left
        # to expire after 7 days.

        if [[ -n "${{ inputs.hf_model }}" ]]; then
          model="${{ inputs.hf_model }}"
          revision="${{ inputs.hf_model_rev }}"
        else
          model="${{ env.DEFAULT_HF_MODEL }}"
          revision="${{ env.DEFAULT_HF_MODEL_REV }}"
        fi
        model_2="${{ inputs.hf_model_2 }}"
        revision_2="${{ inputs.hf_model_2_rev }}"

        python3 tools/download_model.py -m "$model" -r "${revision:-main}" &

        if [[ -n "$model_2" ]]; then
          python3 tools/download_model.py -m "$model_2" -r "${revision_2:-main}" &
        fi

        wait

    - name: "Save HF models cache"
      if: ( steps.changed-src-files.outputs.any_changed == 'true' && github.event_name != 'pull_request' && steps.cache_restore.outputs.cache-hit != 'true' )
      uses: actions/cache/save@v4
      with:
        path: ${{ env.model_path }}
        key: ${{ runner.os }}-hf-model-${{ env.model_key }}

    - name: "Save HF models cache for additional model"
      if: ( steps.changed-src-files.outputs.any_changed == 'true' && inputs.hf_model_2 && github.event_name != 'pull_request' && steps.cache_restore_2.outputs.cache-hit != 'true' )
      uses: actions/cache/save@v4
      with:
        path: ${{ env.model_2_path }}
        key: ${{ runner.os }}-hf-model-${{ env.model_2_key }}

    - name: "Run tests"
      if: steps.changed-src-files.outputs.any_changed == 'true'
      env:
        MASTER_PORT: 12355
        MASTER_ADDR: localhost
        DISTRIBUTED_STRATEGY_IGNORE_MODULES: WordEmbedding
        HF_HUB_OFFLINE: 1
      shell: bash
      run: |
        # Delete the source code so we can ensure we're testing the installed
        # wheel
        rm -fr vllm_spyre
        # We activate .venv manually and run pytest directly instead of using
        # `uv run`, to avoid having `uv run` re-sync any dependencies or
        # re-install the vllm_sypre package from source
        source .venv/bin/activate

        python3 -m pytest ${{ inputs.flags }} \
          tests -v -m "${{ inputs.markers }}"


