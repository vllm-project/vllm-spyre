name: Test

on:
  # Don't use `paths` or `paths-ignore` filter since this workflow is required
  # for all pull requests on main irrespective of file type or location
  # Use `changed-src-files` step to determine if source code was changed
  pull_request:
    # add labeled and unlabeled to the default types (runs when label is added)
    types: [opened, synchronize, reopened, labeled, unlabeled, auto_merge_enabled]
    branches: [main]

  push:
    branches: [main]

  workflow_dispatch:

env:
  FORCE_COLOR: "1"
  VLLM_CPU_DISABLE_AVX512: "true"
  VLLM_TARGET_DEVICE: "empty"
  VLLM_PLUGINS: "spyre"
  HF_HUB_CACHE: "${{ github.workspace }}/.cache/huggingface/hub"
  DEFAULT_HF_MODEL: "ibm-ai-platform/micro-g3.3-8b-instruct-1b"
  DEFAULT_HF_MODEL_REV: "6e9c6465a9d7e5e9fa35004a29f0c90befa7d23f"

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  test:
    timeout-minutes: 20
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest"]
        python_version: ["3.12"]
        vllm_version:
          - name: "default"
            repo: ""
          - name: "vLLM:main"
            repo: "git+https://github.com/vllm-project/vllm --branch main"
        test_suite:
          - name: "chunked prefill"
            markers: "cpu and chunked_prefill and not quantized"
          - name: "static batching"
            markers: "cpu and decoder and not cb and not other_e2e and not chunked_prefill and not quantized"
            flags: "--timeout=300"
            hf_model: "JackFram/llama-160m"
          - name: "fp8"
            markers: "cpu and quantized and multi"
            flags: "--timeout=600 -k 'basic and test_output' --durations=0"
            hf_model: "ibm-ai-platform/micro-g3.3-8b-instruct-1b-FP8"
            hf_model_rev: "0dff8bacb968836dbbc7c2895c6d9ead0a05dc9e"
          - name: "embedding"
            markers: "cpu and embedding and not quantized"
            flags: "--timeout=300"
            hf_model: "sentence-transformers/all-roberta-large-v1"
            hf_model_rev: "cf74d8acd4f198de950bf004b262e6accfed5d2c"
          - name: "scoring"
            markers: "cpu and scoring"
            flags: "--timeout=300"
            hf_model: "cross-encoder/stsb-roberta-large"
            hf_model_rev: "2b12c2c0088918e76151fd5937b7bba986ef1f98"
          - name: "continuous batching"
            markers: "cpu and cb and not quantized"
            flags: "--timeout=300  --durations=0 -s"
          - name: "worker and utils"
            markers: "not e2e and not quantized and not spyre"
            flags: "--timeout=300"
          - name: "compatibility"
            markers: "compat"
            flags: "--timeout=300"
          - name: "other e2e"
            markers: "cpu and other_e2e and not quantized"
            flags: "--timeout=300"
        include:
          # Lower bound support
          - vllm_version:
              name: "vLLM:lowest"
              repo: "git+https://github.com/vllm-project/vllm --tag v0.10.2"
            test_suite:
              name: "backward compat"
              markers: "compat or (cpu and basic)"
              flags: "--timeout=300"
              hf_model_2: "sentence-transformers/all-roberta-large-v1"
              hf_model_2_rev: "cf74d8acd4f198de950bf004b262e6accfed5d2c"
            os: "ubuntu-latest"
            python_version: "3.12"
          # Intermediate versions of vllm to check basic support for as well
          - vllm_version:
              name: "vLLM:0.11.0"
              repo: "git+https://github.com/vllm-project/vllm --tag v0.11.0"
            test_suite:
              name: "backward compat"
              markers: "compat or (cpu and basic)"
              flags: "--timeout=300"
              hf_model_2: "sentence-transformers/all-roberta-large-v1"
              hf_model_2_rev: "cf74d8acd4f198de950bf004b262e6accfed5d2c"
            os: "ubuntu-latest"
            python_version: "3.12"

        # Only run vllm:main jobs on PRs with `vllm:main` label
        exclude: >-
          ${{
            (
              github.event_name != 'pull_request' ||
              !(contains(toJson(github.event.pull_request.labels), '"vllm:main"'))
            ) && fromJSON('[{"vllm_version":{"name":"vLLM:main"}}]')
              || fromJSON('[]')
          }}


    name: "${{ matrix.test_suite.name }} (${{ matrix.vllm_version.name }})"

    steps:
      - name: "Checkout"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: "Run test action"
        uses: ./.github/actions/test
        with:
          python_version: ${{ matrix.python_version }}
          vllm_version_name: ${{ matrix.vllm_version.name }}
          vllm_version_repo: ${{ matrix.vllm_version.repo }}
          markers: ${{ matrix.test_suite.markers }}
          flags: ${{ matrix.test_suite.flags }}
          # TODO: need to conditionally set the model info
